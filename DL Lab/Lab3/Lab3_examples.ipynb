{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ff9e1c0-5922-4eb8-acb1-86e90f48d054",
   "metadata": {},
   "source": [
    "# Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd4b4e7-8653-42e7-a4cf-65a5e4e973b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters are tensor([0.3882], requires_grad=True), and tensor([0.0040], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create the tensors x and y. They are the training\n",
    "# examples in the dataset for the Linear regression\n",
    "x = torch.tensor(\n",
    "[12.4, 14.3, 14.5, 14.9, 16.1, 16.9, 16.5, 15.4, 17.0, 17.9, 18.8, 20.3, 22.4, 19.4, 15.5, 16.7, 17.3, 18.4, 19.2,\n",
    " 17.4, 19.5, 19.7, 21.2])\n",
    "y = torch.tensor(\n",
    "[11.2, 12.5, 12.7, 13.1, 14.1, 14.8, 14.4, 13.4, 14.9, 15.6, 16.4, 17.7, 19.6, 16.9, 14.0, 14.6, 15.1, 16.1, 16.8,\n",
    " 15.2, 17.0, 17.2, 18.6])\n",
    "\n",
    "# The parameters to be learnt w, and b in the\n",
    "# prediction y_p = wx +b\n",
    "b= torch.rand([1], requires_grad=True)\n",
    "w= torch.rand ([1], requires_grad=True)\n",
    "print(\"The parameters are {}, and {}\".format(w, b))\n",
    "\n",
    "# The Learning rate is set to alpha = 0.001 \n",
    "learning_rate = torch.tensor(0.001)\n",
    "\n",
    "# The List of Loss values for the plotting purpose \n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0baf88b-3016-446e-8b96-60de53702aae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters are w=tensor([0.6908], requires_grad=True), b=tensor([0.0210], requires_grad=True), and loss=73.73162841796875\n",
      "The parameters are w=tensor([0.8049], requires_grad=True), b=tensor([0.0275], requires_grad=True), and loss=10.482089042663574\n",
      "The parameters are w=tensor([0.8478], requires_grad=True), b=tensor([0.0299], requires_grad=True), and loss=1.5010334253311157\n",
      "The parameters are w=tensor([0.8640], requires_grad=True), b=tensor([0.0308], requires_grad=True), and loss=0.22577528655529022\n",
      "The parameters are w=tensor([0.8701], requires_grad=True), b=tensor([0.0312], requires_grad=True), and loss=0.04469658434391022\n",
      "The parameters are w=tensor([0.8724], requires_grad=True), b=tensor([0.0313], requires_grad=True), and loss=0.018984422087669373\n",
      "The parameters are w=tensor([0.8733], requires_grad=True), b=tensor([0.0314], requires_grad=True), and loss=0.015333249233663082\n",
      "The parameters are w=tensor([0.8736], requires_grad=True), b=tensor([0.0314], requires_grad=True), and loss=0.014814631082117558\n",
      "The parameters are w=tensor([0.8737], requires_grad=True), b=tensor([0.0314], requires_grad=True), and loss=0.014740866608917713\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0315], requires_grad=True), and loss=0.014730188064277172\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0315], requires_grad=True), and loss=0.014728502370417118\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0315], requires_grad=True), and loss=0.01472808513790369\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0315], requires_grad=True), and loss=0.014727870002388954\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0315], requires_grad=True), and loss=0.014727627858519554\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0315], requires_grad=True), and loss=0.014727408066391945\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0315], requires_grad=True), and loss=0.014727202244102955\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0316], requires_grad=True), and loss=0.014727020636200905\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0316], requires_grad=True), and loss=0.014726840890944004\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0316], requires_grad=True), and loss=0.014726589433848858\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0316], requires_grad=True), and loss=0.014726423658430576\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0316], requires_grad=True), and loss=0.014726169407367706\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0316], requires_grad=True), and loss=0.014725958928465843\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0316], requires_grad=True), and loss=0.014725761488080025\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0317], requires_grad=True), and loss=0.014725537039339542\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0317], requires_grad=True), and loss=0.014725313521921635\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0317], requires_grad=True), and loss=0.014725122600793839\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0317], requires_grad=True), and loss=0.014724953100085258\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0317], requires_grad=True), and loss=0.014724701642990112\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0317], requires_grad=True), and loss=0.014724526554346085\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0317], requires_grad=True), and loss=0.014724315144121647\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0318], requires_grad=True), and loss=0.01472405344247818\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0318], requires_grad=True), and loss=0.014723855070769787\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0318], requires_grad=True), and loss=0.01472365204244852\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0318], requires_grad=True), and loss=0.01472343597561121\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0318], requires_grad=True), and loss=0.0147232785820961\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0318], requires_grad=True), and loss=0.014723068103194237\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0318], requires_grad=True), and loss=0.014722864143550396\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0319], requires_grad=True), and loss=0.014722668565809727\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0319], requires_grad=True), and loss=0.014722442254424095\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0319], requires_grad=True), and loss=0.01472221128642559\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0319], requires_grad=True), and loss=0.014721989631652832\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0319], requires_grad=True), and loss=0.014721833169460297\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0319], requires_grad=True), and loss=0.014721549116075039\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0319], requires_grad=True), and loss=0.014721415936946869\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0320], requires_grad=True), and loss=0.014721174724400043\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0320], requires_grad=True), and loss=0.014720972627401352\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0320], requires_grad=True), and loss=0.014720775187015533\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0320], requires_grad=True), and loss=0.014720549806952477\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0320], requires_grad=True), and loss=0.014720387756824493\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0320], requires_grad=True), and loss=0.014720109291374683\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0320], requires_grad=True), and loss=0.014719943515956402\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0321], requires_grad=True), and loss=0.014719675295054913\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0321], requires_grad=True), and loss=0.014719516038894653\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0321], requires_grad=True), and loss=0.014719278551638126\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0321], requires_grad=True), and loss=0.014719112776219845\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0321], requires_grad=True), and loss=0.014718948863446712\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0321], requires_grad=True), and loss=0.014718662016093731\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0321], requires_grad=True), and loss=0.014718528836965561\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0322], requires_grad=True), and loss=0.01471825409680605\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0322], requires_grad=True), and loss=0.014718055725097656\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0322], requires_grad=True), and loss=0.014717801474034786\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0322], requires_grad=True), and loss=0.014717605896294117\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0322], requires_grad=True), and loss=0.014717419631779194\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0322], requires_grad=True), and loss=0.01471720915287733\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0322], requires_grad=True), and loss=0.014717034995555878\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0323], requires_grad=True), and loss=0.014716794714331627\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0323], requires_grad=True), and loss=0.014716626144945621\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0323], requires_grad=True), and loss=0.014716419391334057\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0323], requires_grad=True), and loss=0.01471622008830309\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0323], requires_grad=True), and loss=0.014715953730046749\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0323], requires_grad=True), and loss=0.014715766534209251\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0323], requires_grad=True), and loss=0.014715534634888172\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0324], requires_grad=True), and loss=0.014715338125824928\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0324], requires_grad=True), and loss=0.014715121127665043\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0324], requires_grad=True), and loss=0.014714973047375679\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0324], requires_grad=True), and loss=0.014714764431118965\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0324], requires_grad=True), and loss=0.014714531600475311\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0324], requires_grad=True), and loss=0.01471435371786356\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0324], requires_grad=True), and loss=0.014714107848703861\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0325], requires_grad=True), and loss=0.014713900163769722\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0325], requires_grad=True), and loss=0.014713701792061329\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0325], requires_grad=True), and loss=0.014713468961417675\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0325], requires_grad=True), and loss=0.01471323985606432\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0325], requires_grad=True), and loss=0.014713084325194359\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0325], requires_grad=True), and loss=0.014712806791067123\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0325], requires_grad=True), and loss=0.014712677337229252\n",
      "The parameters are w=tensor([0.8738], requires_grad=True), b=tensor([0.0326], requires_grad=True), and loss=0.014712407253682613\n",
      "The parameters are w=tensor([0.8737], requires_grad=True), b=tensor([0.0326], requires_grad=True), and loss=0.014712234027683735\n",
      "The parameters are w=tensor([0.8737], requires_grad=True), b=tensor([0.0326], requires_grad=True), and loss=0.014712068252265453\n",
      "The parameters are w=tensor([0.8737], requires_grad=True), b=tensor([0.0326], requires_grad=True), and loss=0.014711814932525158\n",
      "The parameters are w=tensor([0.8737], requires_grad=True), b=tensor([0.0326], requires_grad=True), and loss=0.014711664989590645\n",
      "The parameters are w=tensor([0.8737], requires_grad=True), b=tensor([0.0326], requires_grad=True), and loss=0.014711417257785797\n",
      "The parameters are w=tensor([0.8737], requires_grad=True), b=tensor([0.0326], requires_grad=True), and loss=0.014711247757077217\n",
      "The parameters are w=tensor([0.8737], requires_grad=True), b=tensor([0.0327], requires_grad=True), and loss=0.014710945077240467\n",
      "The parameters are w=tensor([0.8737], requires_grad=True), b=tensor([0.0327], requires_grad=True), and loss=0.014710789546370506\n",
      "The parameters are w=tensor([0.8737], requires_grad=True), b=tensor([0.0327], requires_grad=True), and loss=0.01471055205911398\n",
      "The parameters are w=tensor([0.8737], requires_grad=True), b=tensor([0.0327], requires_grad=True), and loss=0.01471037045121193\n",
      "The parameters are w=tensor([0.8737], requires_grad=True), b=tensor([0.0327], requires_grad=True), and loss=0.014710133895277977\n",
      "The parameters are w=tensor([0.8737], requires_grad=True), b=tensor([0.0327], requires_grad=True), and loss=0.014709925279021263\n",
      "The parameters are w=tensor([0.8737], requires_grad=True), b=tensor([0.0327], requires_grad=True), and loss=0.014709739945828915\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the training Loop for N epochs \n",
    "for epochs in range(100):\n",
    "    # Compute the average Loss for the training samples \n",
    "    loss = 0.0\n",
    "    # Accumulate the Loss for all the samples\n",
    "    for j in range(len(x)):\n",
    "        a = w * x[j]\n",
    "        y_p = a + b\n",
    "        loss +=(y_p-y[j]) ** 2\n",
    "    # Find the average Loss\n",
    "    loss = loss / len(x)\n",
    "    # Add the Loss to a list for the plotting purpose \n",
    "    loss_list.append(loss.item())\n",
    "    # Compute the gradients using backward\n",
    "    # dl/dw and dl/db\n",
    "    loss.backward()\n",
    "    # Without modifying the gradient in this block\n",
    "    # perform the operation\n",
    "    with torch.no_grad():\n",
    "        # Update the weight based on gradient descent\n",
    "        # equivalently one may write w1.copy_(w1 - Learning_rateâœ¶ w1.grad) \n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    # reset the gradients for next epoch\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    # w.grad = None\n",
    "    # b.grad = None\n",
    "    # prev_Loss = Loss\n",
    "    # Display the parameters and Loss\n",
    "    print(\"The parameters are w={}, b={}, and loss={}\".format(w, b, loss.item()))\n",
    "# Display the plot\n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7616f273-2d54-44fc-a42b-d50bbb4e995a",
   "metadata": {},
   "source": [
    "# Linear Regression Model by defining a User Defined Class titled `RegressionModel` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055e0d54-7f6a-44db-b8a2-60c0a392483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel:\n",
    "    def __init__(self):\n",
    "        self.w = torch.rand([1], requires_grad=True)\n",
    "        self.b = torch.rand([1], requires_grad=True)\n",
    "    def forward(self, x):\n",
    "        return self.w * x + self.b\n",
    "    def update(self):\n",
    "        # Update the weight based on gradient descent\n",
    "        # equivalently one may write w1.copy_(w1 - learning_rate * w1.grad)\n",
    "        self.w -= learning_rate * self.w.grad\n",
    "        self.b -= learning_rate * self.b.grad\n",
    "\n",
    "    def reset_grad (self):\n",
    "        self.w.grad.zero_()\n",
    "        self.b.grad.zero_()\n",
    "\n",
    "def criterion(yj, y_p):\n",
    "    return (yj - y_p)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b88cb368-5799-412d-8bbe-8110abf2480a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters are w=tensor([0.7308], requires_grad=True), b=tensor([0.1185], requires_grad=True), and loss=41.75503158569336\n",
      "The parameters are w=tensor([0.8166], requires_grad=True), b=tensor([0.1233], requires_grad=True), and loss=5.94056510925293\n",
      "The parameters are w=tensor([0.8489], requires_grad=True), b=tensor([0.1252], requires_grad=True), and loss=0.8551235198974609\n",
      "The parameters are w=tensor([0.8611], requires_grad=True), b=tensor([0.1259], requires_grad=True), and loss=0.1330212652683258\n",
      "The parameters are w=tensor([0.8657], requires_grad=True), b=tensor([0.1261], requires_grad=True), and loss=0.03048652596771717\n",
      "The parameters are w=tensor([0.8674], requires_grad=True), b=tensor([0.1262], requires_grad=True), and loss=0.015926972031593323\n",
      "The parameters are w=tensor([0.8681], requires_grad=True), b=tensor([0.1263], requires_grad=True), and loss=0.013859566301107407\n",
      "The parameters are w=tensor([0.8683], requires_grad=True), b=tensor([0.1263], requires_grad=True), and loss=0.013565867207944393\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1263], requires_grad=True), and loss=0.013524085283279419\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1263], requires_grad=True), and loss=0.013518044725060463\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1264], requires_grad=True), and loss=0.013517110608518124\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1264], requires_grad=True), and loss=0.013516866602003574\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1264], requires_grad=True), and loss=0.01351670641452074\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1264], requires_grad=True), and loss=0.013516595587134361\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1264], requires_grad=True), and loss=0.013516449369490147\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1264], requires_grad=True), and loss=0.013516303151845932\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1264], requires_grad=True), and loss=0.013516224920749664\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1264], requires_grad=True), and loss=0.013516122475266457\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1264], requires_grad=True), and loss=0.013515975326299667\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1265], requires_grad=True), and loss=0.013515823520720005\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1265], requires_grad=True), and loss=0.013515753671526909\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1265], requires_grad=True), and loss=0.013515590690076351\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1265], requires_grad=True), and loss=0.013515502214431763\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1265], requires_grad=True), and loss=0.013515392318367958\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1265], requires_grad=True), and loss=0.013515237718820572\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1265], requires_grad=True), and loss=0.013515111990272999\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1265], requires_grad=True), and loss=0.013514996506273746\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1265], requires_grad=True), and loss=0.013514900580048561\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1266], requires_grad=True), and loss=0.013514718040823936\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1266], requires_grad=True), and loss=0.013514645397663116\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1266], requires_grad=True), and loss=0.013514524325728416\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1266], requires_grad=True), and loss=0.013514379970729351\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1266], requires_grad=True), and loss=0.013514300808310509\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1266], requires_grad=True), and loss=0.013514123857021332\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1266], requires_grad=True), and loss=0.013514034450054169\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1266], requires_grad=True), and loss=0.01351392176002264\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1266], requires_grad=True), and loss=0.013513810001313686\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1267], requires_grad=True), and loss=0.013513621874153614\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1267], requires_grad=True), and loss=0.01351354829967022\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1267], requires_grad=True), and loss=0.01351341512054205\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1267], requires_grad=True), and loss=0.013513308949768543\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1267], requires_grad=True), and loss=0.013513155281543732\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1267], requires_grad=True), and loss=0.013513036072254181\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1267], requires_grad=True), and loss=0.013512919656932354\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1267], requires_grad=True), and loss=0.013512803241610527\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1267], requires_grad=True), and loss=0.013512714765965939\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1268], requires_grad=True), and loss=0.013512528501451015\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1268], requires_grad=True), and loss=0.01351245492696762\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1268], requires_grad=True), and loss=0.013512329198420048\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1268], requires_grad=True), and loss=0.01351221650838852\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1268], requires_grad=True), and loss=0.013512084260582924\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1268], requires_grad=True), and loss=0.013511965982615948\n",
      "The parameters are w=tensor([0.8685], requires_grad=True), b=tensor([0.1268], requires_grad=True), and loss=0.013511830009520054\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1268], requires_grad=True), and loss=0.0135117182508111\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1268], requires_grad=True), and loss=0.01351159904152155\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1269], requires_grad=True), and loss=0.013511438854038715\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1269], requires_grad=True), and loss=0.013511366210877895\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1269], requires_grad=True), and loss=0.013511227443814278\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1269], requires_grad=True), and loss=0.013511108234524727\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1269], requires_grad=True), and loss=0.013510949909687042\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1269], requires_grad=True), and loss=0.01351087260991335\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1269], requires_grad=True), and loss=0.01351076364517212\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1269], requires_grad=True), and loss=0.0135106286033988\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1269], requires_grad=True), and loss=0.01351051963865757\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1270], requires_grad=True), and loss=0.013510337099432945\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1270], requires_grad=True), and loss=0.013510249555110931\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1270], requires_grad=True), and loss=0.013510137796401978\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1270], requires_grad=True), and loss=0.01351004559546709\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1270], requires_grad=True), and loss=0.013509869575500488\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1270], requires_grad=True), and loss=0.013509770855307579\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1270], requires_grad=True), and loss=0.013509673997759819\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1270], requires_grad=True), and loss=0.013509541749954224\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1270], requires_grad=True), and loss=0.013509382493793964\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1271], requires_grad=True), and loss=0.013509304262697697\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1271], requires_grad=True), and loss=0.013509168289601803\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1271], requires_grad=True), and loss=0.013509063981473446\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1271], requires_grad=True), and loss=0.013508915901184082\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1271], requires_grad=True), and loss=0.013508779928088188\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1271], requires_grad=True), and loss=0.013508671894669533\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1271], requires_grad=True), and loss=0.013508561998605728\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1271], requires_grad=True), and loss=0.01350843533873558\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1271], requires_grad=True), and loss=0.013508295640349388\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1272], requires_grad=True), and loss=0.01350819505751133\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1272], requires_grad=True), and loss=0.013508057221770287\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1272], requires_grad=True), and loss=0.013507972471415997\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1272], requires_grad=True), and loss=0.013507829047739506\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1272], requires_grad=True), and loss=0.013507680967450142\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1272], requires_grad=True), and loss=0.013507568277418613\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1272], requires_grad=True), and loss=0.013507512398064137\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1272], requires_grad=True), and loss=0.013507302850484848\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1272], requires_grad=True), and loss=0.013507203198969364\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1273], requires_grad=True), and loss=0.013507111929357052\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1273], requires_grad=True), and loss=0.013506966643035412\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1273], requires_grad=True), and loss=0.013506869785487652\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1273], requires_grad=True), and loss=0.01350674033164978\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1273], requires_grad=True), and loss=0.013506610877811909\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1273], requires_grad=True), and loss=0.013506474904716015\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1273], requires_grad=True), and loss=0.013506395742297173\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1273], requires_grad=True), and loss=0.013506247662007809\n",
      "The parameters are w=tensor([0.8684], requires_grad=True), b=tensor([0.1273], requires_grad=True), and loss=0.01350612472742796\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = RegressionModel()\n",
    "\n",
    "# The list of loss values for the plotting purpose\n",
    "loss_list = []\n",
    "\n",
    "# Run the training loop for N epochs\n",
    "for epochs in range(100):\n",
    "    loss = 0.0\n",
    "    # Accumulate the loss for all the samples\n",
    "    for j in range(len(x)):\n",
    "        y_p = model.forward (x[j])\n",
    "        loss += criterion(y[j], y_p)\n",
    "    # Find the average loss\n",
    "    loss = loss / len(x)\n",
    "    # Add the loss to a list for the plotting purpose\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "    # Compute the gradients using backward dl/dw and d/db \n",
    "    loss.backward()\n",
    "\n",
    "    # Without modifying the gradient in this block\n",
    "    # perform the operation\n",
    "    with torch.no_grad():\n",
    "        model.update()\n",
    "    # reset the gradients for next epoch\n",
    "    model.reset_grad()\n",
    "    # w.grad = None\n",
    "    # b.grad = None\n",
    "\n",
    "    # prev_loss = loss\n",
    "    # Display the parameters and loss\n",
    "    print(\"The parameters are w={}, b={}, and loss={}\".format(model.w, model.b, loss.item()))\n",
    "    \n",
    "# Display the plot\n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126cf443-56ee-4a54-88e2-c4ce13ebd9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c53e7ea-7565-496b-bc01-3e44ad7f4e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
